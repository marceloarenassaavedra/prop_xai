
\begin{proof}[Proof of~\Cref{prop:hardness}]
    The proof is a twist on ~\citet[Lemma 28]{NEURIPS2020_b1adda14}; let $(s_1, \ldots, s_n, T) \in \mathbb{N}^{n+1}$ be an instance of the $\# \ptime$-complete problem $\#\Knapsack$, that consists on counting the number of sets $S \subseteq \{s_1, \ldots, s_n\}$ such that $\sum_{s \in S}s \leq T$.  
    We can assume that $\sum_{i=1}^n s_i > T$, as otherwise the $\# \Knapsack$ instance is trivial.
    Then, let $\Lin$ be a linear model with weights $w_i = s_i$, and threshold $t = T+1$.
    Now, consider the problem of deciding whether $\#\Knapsack(s_1, \ldots, s_n, T) \geq m$ for an input $m$, which cannot be solved in polynomial time unless $\ptime = \# \ptime$.
    Let $\vx = (1, 1, \ldots, 1)$, and $\delta = \frac{m}{2^{n}}$. We claim that $(\Lin, \vx, \delta, k=0)$ is a Yes-instance for Uniform-Min-$\delta$-SR$(\textsc{Linear})$ if and only if $\#\Knapsack(s_1, \ldots, s_n, T) \geq m$. 
    First, note that $\Lin(\vx) = 1$, since $\sum_{i=1}^n w_i x_i = \sum_{i=1}^n s_i \geq T+1 = t$. 
    For every set $S \subseteq \{s_1, \ldots, s_n\}$ such that $\sum_{s \in S}s \leq T$, its complement $\overline{S} := \{s_1, \ldots, s_n\} \setminus S$ holds $\sum_{s \in \overline{S}}s > T$, and as all values are integers, this implies as well
    \[
        \sum_{s \in \overline{S}} s \geq T+1 = t.
    \]
    To each such set $\overline{S}$, we associate the instance $\vz(\overline{S})$ defined as
    \[
        z(\overline{S})_i = \begin{cases}
            1 & \text{if } s_i \in \overline{S}\\
            0 & \text{otherwise}.
        \end{cases}
    \]
    Now note that 
    \[
        \Lin(\vz(\overline{S})) = \begin{cases}
            1 & \text{if } \sum_{s_i \in \overline{S}} w_i \geq T+1\\
            0 & \text{otherwise} \end{cases} = 1,
    \]
    and thus there is a bijection between the sets $S$ whose sum is at most $T$ and the instances $\vz(\overline{S})$ such that $\Lin(\vz(\overline{S})) = 1 = \Lin(\vx)$.
    To conclude, simply note that the previous bijection implies
    \[
        \Pr_{\vz \sim \mathcal{U}}\Big[\Lin(\vz) = 1\Big] = \frac{\#\Knapsack(s_1, \ldots, s_n, T)}{2^n},
    \] 
    and thus the ``empty explanation'' $(\bot, \bot, \ldots, \bot)$ has probability at least $\delta$ if and only if $\#\Knapsack(s_1, \ldots, s_n, T) \geq m$. As the empty explanation is the only one with size $\leq 0$, we conclude the proof.
    

%     First, recall that the problem of counting the number of ``positive completions'' of a partial instance $\vy$ is $\# \ptime$-hard for linear models~\cite{NEURIPS2020_b1adda14}; that is, given a partial instance $\vy$, a linear model $\Lin$ and a positive integer $K$, it is $\mathrm{PP}$-hard to determine whether there are at least $K$ instances $\vz \in \comp(\vy)$ such that $\Lin(\vz) = 1$. That result follows from the hardness of $\sharpK$.
%    Moreover, in the proof of~\cite{NEURIPS2020_b1adda14} all weights are positive $\delta = \frac{V}{2^{n - |\vy|_\bot}}.$ 
\end{proof}


\begin{proof}[Proof for the locality gap]
    Let us show how  the last case (i.e., $i$ is bad) is shown equationally,
    \[
        \Pr_{\vz \in \D(\vy \ominus i)}[\Lin(\vz) = 1] = \Pr_{\vz \in \D(\vy  \ominus i)}[z_i = y_i] \cdot \Pr_{\vz \in \D(\vy)}[\Lin(\vz) = 1] +   \Pr_{\vz \in \D(\vy  \ominus i)}[z_i \neq y_i] \cdot \Pr_{\vz \in \D(\vy \otimes i)}[\Lin(\vz) = 1]
    \]
    Let $T$ be the threshold for $\Lin$ excluding the defined features in $\vy \ominus i$.
    We can thus rewrite the equation above as:
     
    \begin{multline*}
        \Pr_{\vz \in \D(\vy \ominus i)}\left[\sum_{j\in S} z_j w_j \geq T\right] = \Pr_{\vz \in \D(\vy  \ominus i)}\left[z_i = y_i\right] \cdot \Pr_{\vz \in \D(\vy)}\left[\sum_{j \in S \setminus \{i\}} z_j w_j \geq T- y_i w_i\right]
                                                         \\   +  \Pr_{\vz \in \D(\vy  \ominus i)}\left[z_i \neq y_i\right] \cdot \Pr_{\vz \in \D(\vy \otimes i)}\left[\sum_{j \in S \setminus \{i\}} z_j w_j \geq T - (1-y_i)w_i\right]
\end{multline*}
Now, note that 
\[
    \Pr_{\vz \in \D(\vy)}\left[\sum_{j \in S \setminus \{i\}} z_j w_j \geq T- y_i w_i\right] =   \Pr_{\vz \in \D(\vy)}\left[\Lin(\vz) = 1\right],
\]
and as feature $i$ is bad, we have $w_i(2y_i - 1) < 0$. If $y_i = 0$, then $w_i > 0$, and thus 
\[
    \Pr_{\vz \in \D(\vy)}\left[\sum_{j \in S \setminus \{i\}} z_j w_j \geq T- y_i w_i\right] = \Pr_{\vz \in \D(\vy \otimes i)}\left[\sum_{j \in S \setminus \{i\}} z_j w_j \geq T\right] \leq \Pr_{\vz \in \D(\vy \otimes i)}\left[\sum_{j \in S \setminus \{i\}} z_j w_j \geq T - w_i\right],
\]
and we are done. If $y_i = 1$, then we have $w_i < 0$ and thus $y_i w_i< 0$, from where  
\[
    \Pr_{\vz \in \D(\vy)}\left[\sum_{j \in S \setminus \{i\}} z_j w_j \geq T- y_i w_i\right] \leq \Pr_{\vz \in \D(\vy)}\left[\sum_{j \in S \setminus \{i\}} z_j w_j \geq T\right].
\]
    

\end{proof}