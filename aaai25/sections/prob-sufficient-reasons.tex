The main idea of probabilistic sufficient reasons is to relax the condition \emph{``all completions of the explanation $\vy$ have the same class as $\vx$''} to \emph{``a random completion of $\vy$ has the same class as $\vx$ with high probability''}.

% meaning that an input $\vx$ has probability $\D(\vx)$ of arising. For example, if one were to deploy an algorithm for handwritten-digit classification over $28 \times 28$ binary images (as in a binarized version of MNIST~\cite{deng2012mnist}), then not all $2^{784}$ inputs would be equally likely to appear; those that \emph{look like digits} would be much more likely.
% When considering a partial instance $\vy$, we define $\D_{\vy}$ as the distribution $\D$ conditioned on being a completion of $\vy$, which formally means that 
% \[
% 	\D_{\vy}(\vz) = \frac{\D(\vz)}{\sum_{\vw \in \comp(\vy)} \D(\vw)},
% \]
% for any $\vz \in \comp(\vy)$, and $\D_{\vy}(\vv) = 0$ if $\vv \not\in \comp(\vy)$.
 Let us use notation $\vz \sim \U(\vy)$ to denote that $\vz$ is a completion of $\vy$ drawn uniformly at random.
With this notation we can define $\delta$-sufficient reasons:\footnote{Also known as $\delta$-relevant sets~\cite{Izza2021EfficientEW,Waldchen_MacDonald_Hauch_Kutyniok_2021}.}
\begin{definition}[\cite{Waldchen_MacDonald_Hauch_Kutyniok_2021}]
	For any $\delta \in [0, 1]$, a $\delta$-sufficient reason ($\delta$-SR) for an instance $\vx$, is a partial instance $\vy \subseteq \vx$ such that
	\[
		\Pr_{\vz \sim  \U(\vy)}\big[\M(\vz) = \M(\vx)\big] \geq \delta.
	\]
	\label{def:delta-SR}
\end{definition}
%
Naturally, a minimum $\delta$-SR is a $\delta$-SR of minimum size. Note immediately that~\Cref{def:delta-SR} and~\Cref{def:sufficient-reason} coincide when $\delta = 1$.
\subsection{The size of $\delta$-SRs}

Interestingly, even a $0.999999$-SR can be arbitrarily smaller, in terms of defined features, than the smallest sufficient reason (i.e., $1$-SR) for a pair $(\M, \vx)$, even when $\M$ is a linear model, as we will illustrate in~\Cref{ex:delta-sr-size}. Before providing the example, let us define linear models.
%  and a concentration bound that will be used throughout the paper.

\begin{definition}
	A (binary) linear model $\Lin$ of dimension $d$ is a pair $(\vw, t)$, where $\vw \in \mathbb{Q}^d$ and $t \in \mathbb{Q}$. Its classification over an instance $\vx$ is defined simply as 
	\[
		\Lin(\vx) = \begin{cases}
			1 & \text{if } \vx \cdot \vw \geq t\\
			0 & \text{otherwise}.
		\end{cases}
	\]
	\label{def:linear-models}
\end{definition}

% \begin{lemma}[Chernoff-Hoeffding bound]
% Let $X$ be a finite sum of independent Bernoulli variables, with $\E[X] = \mu > 0$. Then, for any $t \geq 0$, we have
% \[
% \Pr \Big[ \left|X - \mu\right| \geq t \Big] \leq 2\exp\left(\frac{-t^2}{3 \mu} \right).
% \] 

% \label{lemma:chernoff}	
% \end{lemma}

\begin{example}
Consider a linear model $\Lin$ of dimension $d= 1000$ with parameters $t = 1250$ and
$$
	\vw = (
		1000, \, 1, \, 1, \,  1, \,  1, \,  \ldots, \, 1
	).
$$
Let the instance $\vx$ be 
	$(
		1, \, 1, \, 1, \,  1, \,  1, \,  \ldots, \, 1
	),$ so that clearly $\Lin(\vx) = 1$.
One can easily see that any $1$-SR for $\vx$ under $\Lin$ has size $251$, as it must include the first feature and any $250$ other features.
However, if we consider $\vy = (
		1, \, \bot, \, \bot, \,  \bot,  \ldots, \, \bot
	)$, then a simple application of the Chernoff-Hoeffding concentration bound (in the appendix for the completeness) gives that
\[
	\Pr_{\vz \sim \U(\vy)}\Big[ \Lin(\vz) = 1\Big] \geq  0.999999.\]
	This suggests that we might say $\Lin(\vx) = 1$ ``because'' $x_1 = 1$; formally, $\vy$ is a $0.999999$-SR, and $251$ times smaller than any $1$-SR for $\Lin(\vx)$.
\label{ex:delta-sr-size}
\end{example}

In general, if we let $\opt(\M, \vx, \delta)$ denote the size of the smallest $\delta$-SR for $(\M, \vx)$, we can prove the following simple proposition.

\begin{proposition}\label{prop:delta-sr-size}
	Even when restricted to the class of linear models, for any $\delta \in (0, 1]$, and any $\varepsilon > 0$, there are pairs $(\Lin, \vx)$  such that
	\[ 
		\frac{\opt(\Lin, \vx, \delta)}{\opt(\Lin, \vx, \delta-\varepsilon)} = \Omega(d).
	\]
\end{proposition}

It is useful to keep in mind that, in general, the size of minimum $\delta$-SRs is increasing over $\delta$; namely, if $\delta_1 < \delta_2$, then 
\[
	\opt(\M, \vx, \delta_1) \leq \opt(\M, \vx, \delta_2),
\]
for any model $\M$ and instance $\vx$, since any $\delta_2$-SR is also a $\delta_1$-SR. 
