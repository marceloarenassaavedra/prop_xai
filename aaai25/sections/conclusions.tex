We have proved a positive result for the case of linear models, showing that a $(\delta, \varepsilon)$-min-SRs can be computed efficiently, and also a more abstract reason suggesting that linear models might be easier to explain than, e.g., decision trees. However,  a variety of natural questions and directions of research remain open. First, even though the runtime of~\Cref{prop:smoothed-explanation} is polynomial and only has a quasi-linear dependency on $d$, our future work includes lowering the dependency in $1/\varepsilon$ and $1/\gamma$; on a dataset with $d = 500$, setting $\varepsilon = 0.1$ and $\gamma = 0.01$ is already computationally expensive. We acknowledge, in terms of practical implementations, the work of~\citet{Louenas,izza2024locallyminimalprobabilisticexplanations,izzaComputingProbabilisticAbductive2023}. 

Second, our theoretical result has some natural directions for generalization. We considered only binary features, whereas in order to offer a practically useful tool to the community, we will need to understand how to compute (approximate) probabilistic explanations for real-valued features and categorical features. Another  fascinating theoretical question is handling the generalization of our setting to that of product distributions (i.e., feature $i$ takes value $1$ with probability $p_i$ and $0$ otherwise) can also be solved efficiently. A straightforward extension of our techniques would not work and therefore, there is a need for new techniques.  
 
%for instance, a bank that judges loan applications with a linear model might give a lot of weight to the feature \emph{``has been convicted of a financial crime''}, but since the base rate of that features is probably extremely low, then it is probably not a good feature to choose in order to explain why someone who got a loan approved, since leaving that feature out of the explanation should still factor in the ``default value'' for that feature. Theoretically, the issue is that when different features have different weights and different probabilities, it is not clear what the best features to include in an explanation are. This theoretical challenge can be summarized by the following toy problem: 
%\begin{center}
%\emph{Consider $N$ lottery tickets, where ticket $i$ gives a reward $w_i$ with probability $p_i$. If all tickets cost $1$, and you have a budget of $k$, what are the best $k$ tickets to buy in order to maximize the probability of getting a total reward of at least $t$?}
%\end{center}
%Let us discuss some initial thoughts on this problem. The expected value of ticket $i$ is $p_i \cdot w_i$, but buying the top $k$ tickets according to expected value is not optimal; for a minimal example, let $(w_1 = 10, p_1 = 0.5), (w_2 = 1000, p_2 = 0.1), k = 1$; if $t  \leq 10$, then the first ticket is strictly better, but if $t > 10$ the second ticket is strictly better. The point of this example is that the order in which tickets compare to each other depends on $t$.
%Let us show a more interesting example of how the optimal tickets depend on $k$.
%Imagine that for each $i \in \{1, ..., 100\}$ we have $w_i = 100, p_i = 0.5$, and for each $i \in \{101, ..., 200\}$ we have $w_i = 15000, p_i = 0.02$. Fix $t = 1500$, and consider now that if $k = 15$, then it is better to buy only tickets from the second kind (i.e., $i \in \{101, \ldots, 200\}$), as to reach $t=1500$ with tickets from the first kind one would need all 15 of them to win, so the probability would be at most $(1/2)^{15} < 0.02$.  On the other hand, if $k = 40$, then if we buy 40 tickets of the first kind we get $1500$ or more with high probability (use Chernoff's bound), whereas if we buy 40 tickets of the second kind, then the probability of reaching $t = 1500$ is $1 - (0.98)^{40} \approx 0.56$.
%
%Going back to the high level again, it seems that if we want to understand how to explain linear model under product distributions, we need to understand this question; if a bank uses the feature \emph{``has been convicted of a financial crime''}, which when positive severely impacts the probability of the applicant being given a loan, but that feature has a very small probability of being positive over a random applicant, then that is in correspondence to a lottery ticket with a lot of value but low probability; depending on the specific values, it might not be a very explanatory feature, since if one does not include it in the explanation then it is almost safe to assume the applicant will not have that feature anyway. Our future work will focus on understanding this case, and more in general, on understanding the computational limits of explainability over linear models.