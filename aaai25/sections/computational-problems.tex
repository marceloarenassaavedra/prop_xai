\citet{Waldchen_MacDonald_Hauch_Kutyniok_2021} showed that computing the smallest $\delta$-SR for arbitrary classifiers is hard for $\textrm{NP}^{\textrm{PP}}$, and that no algorithm can achieve an approximation factor (in terms of $k^\star$) of $d^{1-\alpha}$, $\alpha > 0$, unless $\ptime = \textrm{NP}$. Aftewards,~\citet{Arenas_Barcelo_Romero_Subercaseaux_2022} showed that even for the restricted class of decision trees, usually considered the interpretable, smallest $\delta$-SRs cannot be computed unless $\ptime = \textrm{NP}$, and furthermore,~\citet{Kozachinskiy_2023} proved that the approximation task is also hard for decision trees. Notably, none of these hardness results rely on the distribution $\D$ being complicated, as they were proved taking $\D = \textrm{Uniform}(\{0, 1\}^d)$. 
In stark contrast, we will be able to provide positive approximability results in terms of $\delta$ for linear models, which is, to the best of our knowledge, the first positive theoretical result in the area.
Let us formally define the computational problem at hand, assuming the uniform distribution over $\{0, 1\}^d$, which we will simply denote by $\mathcal{U}$ from now on.



 \csproblem{Uniform-Min-$\delta$-SR$(\mathcal{C})$}{a model $\M \in \mathcal{C}$, an instance $\vx$, a value $\delta \in [0, 1]$, and an integer $k \geq 0$.}{\textsc{\bf Yes} if $k^\star(\M, \vx, \delta) \leq k$, and \textsc{\bf No} otherwise.}
 
Unfortunately, it turns out that even Uniform-Min-$\delta$-SR$(\textsc{Linear})$ is hard, as the amount
\[
    \Pr_{\vz \sim  \mathcal{U}_\vy}\Big[\Lin(\vz) = 1\Big]
\]
cannot be computed in polynomial time unless $\ptime=\# \ptime$~\cite{NEURIPS2020_b1adda14}. 

\begin{proposition}\label{prop:hardness}
    Uniform-Min-$\delta$-SR$(\textsc{Linear})$ cannot be solved in polynomial time unless $\ptime = \# \ptime$.
\end{proposition}


We will consider the problem of approximating the minimum size $\delta$-SRs for linear models under the uniform distribution. To do this, let us consider two senses in which one can approximate $\delta$-SRs.
\begin{itemize}
    \item \textbf{Approximation in terms of $\delta$}: Given a linear model $\Lin$, an instance $\vx$, and a value $\delta$, we want to compute a $\delta'$-SR of size $k^\star(\Lin, \vx, \delta')$ such that $\delta'$ is close to $\delta$. Intuitively, this corresponds to the idea of stakeholders not caring about the exact value of $\delta$; e.g., $90\%$ of completions of $\vy$ agree with $\vx$ has roughly the same human implications as $89.9997\%$.
    \item \textbf{Approximation in terms of $k^\star$}: Given a linear model $\Lin$, an instance $\vx$, and a value $\delta$, we want to compute a $\delta$-SR of size $k$ such that $k$ is not much bigger than $k^\star(\Lin, \vx, \delta)$. Intuitively, even though stakeholders want \emph{small} explanations, it is not required to find the smallest possible explanation.
\end{itemize}

Furthermore, we define a slighty different sense in which the value of $\delta$ can be relaxed:

\begin{definition}[$(\delta, \varepsilon)$-relaxed minimum SR]
    Given a model $\M$, an instance $\vx$, and values $\delta, \varepsilon \in (0, 1)$, we say a partial instance $\vy$ of size $k$ is a $(\delta, \varepsilon)$-relaxed minimum SR if the two following conditions hold:
    \begin{enumerate}
        \item The partial instance $\vy$ is close enough to a $\delta$-SR:
        \[
            \Pr_{\vz \sim \D_{\vy}}\big[\M(\vz) = \M(\vx)\big] \geq \delta - \varepsilon.
        \]
        \item No smaller partial instance $\vy'$ is a clear candidate to be a $\delta$-SR:
        \[
            \Pr_{\vz \sim \D_{\vy'}}\big[\M(\vz) = \M(\vx)\big] < \delta - 2\varepsilon, \quad \forall \vy', |\vy'|_e < |\vy|_e.
        \]
    \end{enumerate}
\end{definition}


With the previous notions, we can state two results and two open problems.
\begin{theorem}\label{thm:delta-approximation}
    The Uniform-Min-$\delta$-SR$(\textsc{Linear})$ problem admits an FPRAS with respect to $\delta$. In particular, there exists an algorithm that computes a minimum $\delta'$-SR for some $\delta' \in [\delta, \delta + \varepsilon]$, and succeeds with probability at least $1-\gamma$, running in time $\tilde{O}\left(\frac{d^3}{\varepsilon^2\gamma^2}\right)$.
\end{theorem}

\begin{theorem}\label{thm:delta-relaxed}
    We can compute a $(\delta, \varepsilon)$-relaxed minimum SR in polynomial time for linear models under the uniform distribution, with a runtime of 
    \[ 
        \tilde{O}\left(\frac{d}{\varepsilon^2\gamma^2}\right).
    \]
\end{theorem}

\begin{open}
    Is there an FPRAS with respect to $k^\star$ for  Uniform-Min-$\delta$-SR$(\textsc{Linear})$?
\end{open}

\begin{open}
    Is there an FPRAS with respect to $\delta$ for Product-Min$\delta$-SR$(\textsc{Linear})$? That is, under product distributions in which each feature $i$ has an associated probability $p_i$ of taking value $1$ and these are all independent.
\end{open}

% \begin{theorem}\label{thm:k-approximation}
%     The Uniform-Min-$\delta$-SR$(\textsc{Linear})$ problem can be approximated over $k^\star$ with additive error $1$ and probability of success $1-\gamma$ in time $\tilde{O}\left( \frac{d^2}{\varepsilon^2\gamma^2}\right)$.
% \end{theorem}

In order to prove~\Cref{thm:delta-approximation,thm:delta-relaxed} we will need two main ideas: first, the fact that we can estimate the probabilities of linear models accepting a partial instance through sampling, and second, that under the uniform distribution it is easy to decide which features ought to be part of small explanations.

\subsection{Estimating the Probability of Acceptance}
 The  hardness of computing
$\Pr_{\vz  \in \D(\vy)}[\M(\vz) = 1]$ is about computing it to arbitrarily high precision, i.e., with an additive error within $O(2^{-d})$. However, computing a less precise estimation of $\Pr_{\vz \in \D(\vy)}[\M(\vz) = 1]$ is simple, as the next fact (which is a direct consequence of Hoeffding's inequality)  states.

\begin{fact}\label{fact:hoeffding}
    Let $f$ be an arbitrary boolean function on $n$ variables. Let $M$ be any positive integer,
    and let $\vx_1, \ldots, \vx_M$ be $M$ uniformly random samples from $\{0, 1\}^n$. Then 
    \[
        \hat{\mu} := \frac{\sum_{i=1}^M [f(\vx_i) = 1]}{M}
    \]
    is an unbiased estimator for 
    \[
        \mu := \Pr_{\vx \in \{0, 1\}^n}[f(\vx) = 1],
    \]
    and 
    \[
    \Pr[\left|\hat{\mu} - \mu \right| \leq t] \geq 1 - 2e^{-2t^2 M},
    \]
    which is at least $1 - \gamma$ for $M = \frac{1}{2t^2} \log(2/\gamma)$.
\end{fact}

 \paragraph{Smoothed Explanations} As a consequence of the previous idea, although a minimum $\delta$-SR might be hard to compute, this crucially depends on the value of $\delta$. In the spirit of smoothed analysis, we define the computation of a min-$(\delta, \varepsilon)$-SR as follows: first, a value $\delta^\star$ is chosen uniformly at random from $[\delta-\varepsilon, \delta+\varepsilon]$, and then a min-$\delta^\star$-SR is computed. Intuitively, the idea is that as $\delta^\star$ is chosen at random, it will be unlikely that a value that makes the computation hard is chosen. 

 We will prove the following proposition:

\begin{proposition}
    \label{prop:smoothed-explanation}
    Given a linear model $\Lin$ and an input $\vx$, we can compute a $(\delta, \varepsilon)$-SR successfully with probability $1 - \gamma$ in time polynomial in $d$, $1/\varepsilon$ and $1/\gamma$. In particular, in time $\tilde{O}\left( \frac{d^2}{\varepsilon^2\gamma^2}\right)$.
\end{proposition}

Note that~\Cref{prop:smoothed-explanation} immediately gives us~\Cref{thm:delta-approximation}, as we can set $\delta' =  \delta+\varepsilon/2$ and $\varepsilon' = \varepsilon/2$, which means with probability $1 - \gamma$ we will obtain a $(\delta', \varepsilon')$-SR, whose probability guarantee is in 
\[
  [\delta' - \varepsilon', \delta' + \varepsilon'] =  [\delta, \delta + \varepsilon].
\]

Furthermore, if one were to prefer explanations that err on the opposite side, meaning that their probability is in $[\delta - \varepsilon, \delta]$, our results would hold the same, by simply setting $\delta' = \delta - \varepsilon/2$, $\varepsilon' = \varepsilon/2$, and naturally these would be guaranteed to be smaller in size that the minimum $\delta$-SR.

Before proving~\Cref{prop:smoothed-explanation}, we need to prove a lemma concerning the easiness of selecting the features of the desired explanation.

\subsection{Feature Selection}

Even if we were granted an oracle computing the probabilities
\(
    \Pr_{\vz  \in \D(\vy)}[\M(\vz) = 1]
\), that would not be necessarily enough to efficiently compute the minimum $\delta$-SR. Indeed, for decision trees, the counting problem can be easily solved in polynomial time~\cite{NEURIPS2020_b1adda14}, and yet the computation of $\delta$-SRs of minimum size is hard, even to approximate~\cite{Arenas_Barcelo_Romero_Subercaseaux_2022,Kozachinskiy_2023}.
Intuitively, the problem for decision trees is that, even if we were told that the minimum $\delta$-SR has exactly $k$ features, it is not obvious how to search for it better than enumerating all $\binom{d}{k}$ subsets. The case of linear models, however, is different, at least under the uniform distribution. In this case, every feature $i$ that is not part of the explanation will take value $0$ or $1$ independently with probability $\nicefrac{1}{2}$, and contribute to the classification according to its weight $w_i$. In other words, we can sort the features according to their weights (with some care about signs), and select them greedily to build a small $\delta$-SR. A proof for the deterministic case ($\delta = 1$) was already given in~\cite{NEURIPS2020_b1adda14} and sketched earlier on by~\cite{ExplainingNaiveBayes}.

\begin{definition}\label{def:scores}
    Given a linear model $\Lin = (\vw, t)$, and an instance $\vx$, both having dimension $d$, we define the \emph{score} of feature $i \in [d]$ as 
    \[
        s_i := w_i \cdot (2x_i - 1) \cdot (2\left(\Lin(\vx) - 1\right)).    
    \]
\end{definition}

In other words, the sign of $s_i$ is $+1$ if the feature is ``helping'' the classification, and $-1$ if it is ``hurting'' it. The magnitude of $s_i$ is proportional to the weight of the feature $i$. Changing the value of feature $i$ in an instance $\vx$ would decrease $\vw \cdot \vx$ by $s_i$ if $\Lin(\vx) = 1$, and increase it by $s_i$ if $\Lin(\vx) = 0$.
For the uniform distribution, we can prove the following lemma that basically states that, if we are promised a $\delta$-SR of size $k$ exists, then one can be found by taking the top-$k$ features of maximum score $s_i$.

\begin{lemma}\label{lemma:greedy}
Given a linear model $\Lin$, an instance $\vx$,  a value $\delta \in (0, 1]$, and an integer $k$, then there exists a $\delta$-SR for $\vx$ under $\Lin$ of size $k$ if and only if the partial instance defined only on the $k$ features of $\vx$ with maximum score is a $\delta$-SR for $\vx$ under $\Lin$.
\end{lemma}

Even though a proof of~\Cref{lemma:greedy} is presented in the appendix, let us provide a self-contained example that should convince a reader of the veracity of the lemma.
\begin{example}\label{ex:greedy}
 Consider an instance $\vx = (1,\, 0, \, 0, \, 1, \, 1)$ and the linear model $\Lin$ be defined by 
 \[ 
    \vw = (5, \, 1, \, -3,\, 2, -1) \quad ; \quad t = 5.
 \]
 It is easy to check that $\vw \cdot \vx = 5$, and thus $\Lin(\vx) = 1$. The feature scores, according to~\Cref{def:scores}, are:
 \[
  s_1 = 5, \; s_2 = -1, \; s_3 = 3, \; s_4 = 2, \; s_5 = -1.
 \]
Consider the partial instances $\vy^{(1)} = (\bot, \, 0, \, 0, \, 1, \, 1)$ and $\vy^{(2)} = (1, \, \bot, \, 0, \, 1, \, 1)$. The instance $\vx$ is a completion of both $\vy^{(1)}$ and $\vy^{(2)}$, but $\vy^{(1)}$ also has completion 
\[
    \vx^{(1)} = (0, \, 0, \, 0, \, 1, \, 1),
\]
whereas $\vy^{(2)}$ has also completion 
\[
    \vx^{(2)} = (1, \, 1, \, 0, \, 1, \, 1).
\]
Note that $\vw \cdot  \vx^{(1)} = 1 = \vw \cdot \vx - s_1$, whereas $\vw \cdot  \vx^{(2)} = 6 = \vw \cdot \vx - s_2$. Intuitively, this means that it is better to keep feature $1$ as part of the explanation, but not feature $2$. If we want an explanation with only two features, we should choose feature $1$ and feature $3$, as they have the highest scores. Indeed,~\Cref{table:ex-greedy} presents the probabilities to all possible explanations of size $2$.
\begin{table}
    \caption{Table of probabilities associated to~\Cref{ex:greedy}.}\label{table:ex-greedy}
    \centering
    \begin{tabular}{rrr}
        \toprule
        Partial instance & Features included & Probability\\ \midrule 
    $(1, \,\bot, \,0, \,\bot, \,\bot)$ & $\{1, \,3\}$ & $\nicefrac{ 7 }{ 8 }$\\
    $(1, \,\bot, \,\bot, \,1, \,\bot)$ & $\{1, \,4\}$ & $\nicefrac{ 5 }{ 8 }$\\
    $(\bot, \,\bot, \,0, \,1, \,\bot)$ & $\{3, \,4\}$ & $\nicefrac{ 1 }{ 2 }$\\
    $(\bot, \,\bot, \,0, \,\bot, \,1)$ & $\{3, \,5\}$ & $\nicefrac{ 3 }{ 8 }$\\
    $(\bot, \,0, \,0, \,\bot, \,\bot)$ & $\{2, \,3\}$ & $\nicefrac{ 3 }{ 8 }$\\
    $(1, \,\bot, \,\bot, \,\bot, \,1)$ & $\{1, \,5\}$ & $\nicefrac{ 3 }{ 8 }$\\
    $(1, \,0, \,\bot, \,\bot, \,\bot)$ & $\{1, \,2\}$ & $\nicefrac{ 3 }{ 8 }$\\
    $(\bot, \,\bot, \,\bot, \,1, \,1)$ & $\{4, \,5\}$ & $\nicefrac{ 1 }{ 4 }$\\
    $(\bot, \,0, \,\bot, \,1, \,\bot)$ & $\{2, \,4\}$ & $\nicefrac{ 1 }{ 4 }$\\
    $(\bot, \,0, \,\bot, \,\bot, \,1)$ & $\{2, \,5\}$ & $\nicefrac{ 1 }{ 8 }$\\
        \bottomrule
    \end{tabular}
\end{table}
\end{example}

% \todo[inline]{Bernardo: The proof of this is trivial but annoying, it suffices to do an exchange argument. I will write it down later.}

With~\Cref{lemma:greedy} in hand, we can now prove~\Cref{prop:smoothed-explanation}.

\input{proof-of-thm1.tex}


% Let us restate and prove our other theorem.

% \begin{theorem}
%     The Uniform-Min-$\delta$-SR$(\textsc{Linear})$ problem can be approximated over $k^\star$ with additive error $1$ and probability of success $1-\gamma$ in time $\tilde{O}\left( \frac{d^2}{\varepsilon^2\gamma^2}\right)$.
% \end{theorem}
% \begin{proof}[Proof sketch]
%     Sort the features according to \emph{score} again, then we try to find the minimum $k$ such that the top-$k$ most important features give a probability of $\delta$. In particular, we will accept the first $k$ such that the empirical probability is above $\delta + (1-\delta)/2d$. The chances of the ``true'' probability being above $\delta$ will naturally be good, the problem is to guarantee that we won't overshoot by much. The main insight here is that, if $k^*$ is the true optimal answer (thus having probability at least $\delta$), then $k^*+1$ should always have the desired probability; note that the remaining probability is $1-\delta$, and there are $d-{k^*}$ features remaining, so the next most important one must yield at least a gain of 
%     $\frac{1-\delta}{d - k^*} \geq \frac{1 - \delta}{d}$. So by sampling with an error of $\frac{1-\delta}{2d}$ we will recognize this, which by using the fact requires 
% $
%     \tilde{O}\left(\left(\frac{1-\delta}{2d}\right)^2\right)
% $ many samples to ensure with high probability. Note that the problematic case is now when $\delta \approx 1$. 
% \end{proof}
