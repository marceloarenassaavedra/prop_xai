\renewcommand{\algorithmiccomment}[1]{\; \; \texttt{/* #1 */}}
\begin{algorithm}[tb]
	\caption{Uni-Linear MonteCarlo}
	\label{alg:algorithm}
	\textbf{Input}: Linear model $\Lin$, instance $\vx$, parameters $\delta \in (0, 1)$\\
	\textbf{Parameter}:  $\varepsilon \in (0, 1)$,  $\gamma \in (0, 1)$\\
	\textbf{Output}: A value $\delta^\star \in [\delta, \delta+\varepsilon]$ together with an optimal $\delta^\star$-SR explanation for $\vx$.
	\begin{algorithmic}[1] %[1] enables line numbers
	\STATE Sample $\delta^\star$ uniformly at random from $[\delta, \delta + \varepsilon]$
	\STATE Compute the score $s_i$ for each feature
	\STATE Let $\vec{\mathcal{F}}$ be the sequence of pairs $(i, s_i)$ sorted decreasingly by $s_i$
	\STATE Let $\ell$ be the max value such that $\vec{\mathcal{F}}[\ell] = (\lambda, s_\lambda)$ has $s_\lambda > 0$.
	\STATE For $k \in [1, \ell]$, let $\vy^{(k)} \subseteq \vx$ be the partial instance defined only in features $\vec{\mathcal{F}}[1][1], \ldots, \vec{\mathcal{F}}[k][1]$.
	\STATE Let $\textrm{LB} = 0$, and $\textrm{UB} = \ell$.
	\STATE $M \gets \frac{2\log d^2}{\varepsilon^2 \gamma^2} \log(2 \log d / \gamma)$.

	\WHILE[Binary Search]{$\textrm{LB} \neq \textrm{UB}$} 
	% \COMMENT{Binary Search}
		\STATE $m \gets \left(\textrm{LB} + \textrm{UB} \right)/2$.
		\STATE $\hat{v}_m \gets \textsc{MonteCarloSampling}(\Lin, \vy^{(m)}, M)$.
		\IF {$\hat{v}_m \geq \delta^\star$}
			\STATE $\textrm{UB} \gets m$
		\ELSE
			\STATE $\textrm{LB} \gets (m+1)$
		\ENDIF
	\ENDWHILE
	\STATE $k^\star \gets \textrm{LB}$ (or equivalently, $\textrm{UB}$).
	\STATE \RETURN $(\delta^\star, \vy^{(k^\star)})$
	\end{algorithmic}
	\end{algorithm}
\begin{proof}


We use~\Cref{alg:algorithm}. Let us define the values $v_k$ as
	\[
		v_k := \Pr_{\vz \in \textsc{Comp}(\vy^{(k)})}[\Lin(\vz) = 1].
	\]
As a result of the binary search (lines 8-16), the algorithm obtains $k^\star$, the smallest $k$ such that
\[
	\hat{v}_k \geq \delta^\star,
\]
and our goal is to show that with good probability $k^\star$ is also the smallest $k$ such that $v_k \geq \delta^\star$, which would imply the correctness of the algorithm.
Let $S$ be a random variable corresponding to the set of values $k$ such that~\Cref{alg:algorithm} enters line $10$ with $m=k$, and note that if for every $k$ in $S$ it happens that the events 
\[
	A_k := \left(v_k \geq \delta^\star \right) \text{ and }  B_k := \left(\hat{v_k}(M) \geq \delta^\star\right)
\]
are equivalent (i.e., either both occur or neither occurs), then the algorithm will succeed, as that would indeed imply that $k^\star$ is the smallest $k$ such that $v_k \geq \delta^\star$. 

Then, define events $E_k$ and $F_k$ as follows:
\begin{align*}
	E_k &:= |\delta^\star - v_k| > \frac{\epsilon \gamma}{\log d},\\
	F_k &:= k \in S \land |\hat{v_k}(M) - v_k| \leq \frac{\epsilon \gamma}{\log d}.
\end{align*}
We claim that if both $E_k$ and $F_k$ hold for some $k$, then $A_k$ and $B_k$ are equivalent events. Indeed,
\begin{align*}
	A_k &\iff v_k \geq \delta^\star\\
		&\iff v_k \geq \delta^\star + \frac{\epsilon \gamma}{\log d} \tag{By $E_k$}\\
		&\iff v_k - \frac{\epsilon \gamma}{\log d} \geq \delta^\star\\
		&\iff \hat{v_k}(M) \geq \delta^\star \tag{By $F_k$}\\
		&\iff B_k.
\end{align*}

Thus, if we show that $E_k$ and $F_k$ hold with good probability for every $k \in S$, we can conclude the theorem.
To see that, note first that for every $k \in [d]$, line 1 implies
\[
	\Pr[\overline{E_k}] = \Pr\left[\delta^\star \in \left[v_k - \frac{\epsilon \gamma}{\log d}, v_k + \frac{\epsilon \gamma}{\log d}\right]\right] \leq \frac{\frac{2\epsilon \gamma}{\log d}}{2\epsilon} = \frac{\gamma}{\log d}.
\]

It is tempting to say that we have 
\[ 
	\Pr\left[\bigcap_{k \in S} E_k\right] = 1 -  \Pr\left[\bigcup_{k \in S} \overline{E_k} \right] \geq 1 - \gamma,
\]
through a union bound on the $\log d$ elements of $S$\footnote{For simplicity we will say $|S| \leq \log d$, even though the exact bound for a binary search is $|S| \leq \lfloor \log d  + 1 \rfloor$.}. However, $S$ itself is a random variable, and $E_k$ and $k \in S$ are not (necessarily) independent events, so we need to be more careful. 
Using the law of total probabilities, we have 
\begin{align*}
	\Pr\left[\bigcap_{k \in S} E_k\right] &= \sum_{S' \subseteq [d]} \Pr\left[S = S'  \mid \bigcap_{k \in S'} E_k \right] \Pr\left[\bigcap_{k \in S'} E_k\right]\\
	&= \sum_{\substack{S' \subseteq [d]\\ |S'| \leq \log d}} \Pr\left[S = S' \mid \bigcap_{k \in S'} E_k\right] \Pr\left[\bigcap_{k \in S'} E_k\right],
\end{align*}
where we can now effectively use the union bound to say that for any fixed $S'$ with $|S'| \leq \log d$ we have
\[ 
	\Pr\left[\bigcap_{k \in S'} E_k \right] \geq 1 - \gamma,
\]
from where we get
\begin{align*}
	\Pr\left[\bigcap_{k \in S} E_k\right] &=  \sum_{\substack{S' \subseteq [d]\\ |S'| \leq \log d}} \Pr\left[S = S'\mid \bigcap_{k \in S'} E_k\right] \Pr\left[\bigcap_{k \in S'} E_k\right]\\
	&\geq (1-\gamma) \sum_{\substack{S' \subseteq [d]\\ |S'| \leq \log d}} \Pr\left[S = S' \mid \bigcap_{k \in S'} E_k\right]\\
	&= 1 - \gamma.
\end{align*}
 Let us now argue that $F_k$ holds with good probability for every $k \in S$. Indeed, we have for any $k$ that
 \begin{align*}
 \Pr\left[ \, \overline{F_k} \mid k \in S \right] &= \Pr\left[|\hat{v_k}(M) - v_k| > \frac{\epsilon \gamma}{\log d}\right]\\
 	&\leq \frac{\gamma}{\log d}, \tag{By~\Cref{fact:hoeffding} and line 7.}
 \end{align*}
from where a final union bound (using the same trick as for $E_k$) yields 
\begin{align*}
	\Pr\left[\bigcap_{k \in S} F_k\right] &= 1 - \Pr\left[\bigcup_{k \in S} \overline{F_k}\right]\\
	&\geq 1 - \sum_{k \in S} \Pr\left[\overline{F_k} \mid k \in S\right]\\
	&\geq 1 - \sum_{k \in S} \frac{\gamma}{\log d}\\
	&= 1 - \gamma.
\end{align*}
Therefore, the algorithm will succeed with probability at least 
\[ 
	\Pr\left[\bigcap_{k \in S} E_k\right] \cdot \Pr\left[\bigcap_{k \in S} F_k\right] \geq (1-\gamma)^2 \geq 1-2\gamma.
\]
The runtime is simply
$O(\log d \cdot M \cdot d  )$; as (i) the binary search performs $O(\log d)$ steps; (ii) each of the binary search steps requires $M$ samples, and (iii) each sample requires evaluating the model $\Lin$ and thus takes time $O(d)$. Naturally, running the algorithm with $\gamma' = 1/2 \cdot \gamma$ will yield a success probability of $1-\gamma$ without changing the asymptotic runtime, and thus we conclude the proof.


% We thus want to lower bound the probability of some decently large value $\Delta$ such that $|\delta^\star - v_k| \geq \Delta$ for every $k \in [d]$; that way, it will be enough to estimate the values $v_k$ up to an additive error of $\Delta/2$. Given that there are $d$ values $v_
% k$, and each of them forbids a segment of size $2\Delta$ for $\delta^\star$, then the probability of choosing a value of $\delta^\star$ such that $|\delta^\star - v_k| \geq \Delta$ for every $k \in [d]$ is at least
% \[
% 	1 - \frac{d\Delta}{\epsilon}.
% \]
% As we want our algorithm to succeed with probability $1 - \gamma$, that requires 
% \(
% \frac{d\Delta}{\epsilon} \leq \gamma
% \)
% and thus
% \[
% 	\Delta \leq \frac{\epsilon \gamma}{d}.
% \]
% Thus we need to sample with an additive error smaller than
% \(
% \frac{\epsilon}{2d \gamma},
% \)
% which requires the number of samples $M$ to be such that
% \[
% 	\frac{\log M}{\sqrt{M}} \leq \frac{\epsilon\gamma}{2d}.
% \] 
% %%Considering that
% %%\[
% %%	\frac{\log M}{\sqrt{M}} \leq \frac{\log M}{\log M \sqrt[2+\gamma]{M}} = \frac{1}{\sqrt[2+\gamma]{M}},
% %%\]
% %Let us try to simplify things by considering establishing
% %\[
% %	\frac{1}{\sqrt{M'}} \leq \frac{\epsilon\gamma}{2d}, 
% %\]
% %or equivalently,
% %\[
% %	M \geq \left( \frac{2d}{\epsilon\gamma} \right)^2.
% %\]
% Now we pose
% \[
% 	M =	9\cdot \left( \frac{2d}{\epsilon\gamma} \right)^2\log\left(\frac{2d}{\epsilon\gamma}\right)^2,
% \]
% so that $\log M \leq 3\log\left(\frac{2d}{\epsilon\gamma}\right)$, and thus
% \[
% 	\frac{\log M}{\sqrt{M}} \leq \frac{3\log(2/(\epsilon\gamma))}{3\left( \frac{2d}{\epsilon\gamma} \right)\log\left(\frac{2d}{\epsilon\gamma}\right)} = \frac{\epsilon\gamma}{2d},
% \]
% as desired.

% Thus, the algorithm above, which requires a sampling process at most $\log d$ times, using binary search to find the min, can be carried out with probability of success at least $1-\gamma$. in time 
% \[
% O(\log d \cdot M) = \tilde{O}\left(\left(\frac{d}{\epsilon\gamma}\right)^2\right).
% \]


% \begin{figure}
% 	\centering
% 	\scalebox{0.8}{
% 	\begin{tikzpicture}
% 		\node[] (beg) at (0, 0) {};
% 		\node[] (end) at (11, 0) {};
% 				\node[] (a) at (10, 0.5) {\small $1$};
% 		\node[] (amark) at (10, 0) {\small $\mid$};
% 		\node[] (b) at (5, 0.5) {\small $\nicefrac{1}{2}$};
		
% 		\node[] (b) at (0, 0.5) {\small $0$};
		
% 		\node[] (bmark) at (5, 0) {\small $\mid$};
		
% 		\node[] (leftPar) at (5.5, 0) {\textcolor{blue}{$\Big[$}};
% 		\node[] (rightPar) at (9.5, 0) {\textcolor{blue}{$\Big]$}};
		
% 		\draw[-, blue, dashed, very thick]  (leftPar) -- (rightPar);
		
% 		\draw[|-, very thick] (beg) -- (leftPar);
% 			\draw[->, very thick] (rightPar) -- (end);

		
% 		\node[] (left) at (5.5, -0.7) {\textcolor{blue}{\scriptsize $\delta-\varepsilon$}};
% 		\node[] (right) at (9.5, -0.7) {\textcolor{blue}{\scriptsize $\delta+\varepsilon$}};
		
% 		\node[] (delta) at (7.5, -0.5) {\textcolor{blue}{$\delta$}};
		
% 		\node[] (dmark) at (7.5, 0) {\textcolor{blue}{\small $\mid$}};
		
		
% 		\node[] (v1) at (8.8, 0) {\textcolor{purple}{\small $\mid$}};
% 		\node[] (v1) at (8.8, -0.4) {\textcolor{purple}{\scriptsize $v_1$}};
		
% 		\node[] (v2) at (6.6, 0) {\textcolor{purple}{\small $\mid$}};
% 		\node[] (v2) at (6.6, -0.4) {\textcolor{purple}{\scriptsize $v_2$}};
		
% 		\node[] (v4) at (3.6, 0) {\textcolor{purple}{\small $\mid$}};
% 		\node[] (v4) at (3.6, -0.4) {\textcolor{purple}{\scriptsize $v_3$}};
		
% 		\node[] (v3) at (1.2, 0) {\textcolor{purple}{\small $\mid$}};
% 		\node[] (v3) at (1.2, -0.4) {\textcolor{purple}{\scriptsize $v_4$}};
		
% 		\draw[rectangle, fill=purple, line width=0pt] (6.3, -0.2) -- (6.3, 0.2) -- (6.9, 0.2) -- (6.9, -0.2) -- (6.3, -0.2);
		
% 		\draw[rectangle, fill=purple, line width=0pt] (8.5, -0.2) -- (8.5, 0.2) -- (9.1, 0.2) -- (9.1, -0.2) -- (8.5, -0.2);
		
		
% 		\node at (6.6, 0.65) {$\Delta$};
% 		\node at (8.8, 0.65) {$\Delta$};
% 		\draw[decorate, decoration={brace, amplitude=3pt}, thick] (6.3, 0.28) -- (6.9, 0.28);
		
% 			\draw[decorate, decoration={brace, amplitude=3pt}, thick] (8.5, 0.28) -- (9.1, 0.28);
		
% 		\node[] (deltastar) at (8.1, -1.2) {\large \textcolor{violet}{$\delta^\star$}};
% 		\draw[-, , violet, thick] (8.0, -0.9) -- (8.0, 0.9);

 
% 	\end{tikzpicture}
% 	}
% 	\caption{Illustration of the proof, using $d=4, \delta = \nicefrac{3}{4}$ as an example. In this case, the probability of failure is $\frac{2\Delta}{\varepsilon}.$}
% \end{figure}

\end{proof}