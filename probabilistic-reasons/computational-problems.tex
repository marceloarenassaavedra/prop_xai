\section{Computational Problem}

\cite{Waldchen_MacDonald_Hauch_Kutyniok_2021} showed that computing the smallest $\delta$-SR for arbitrary classifiers is hard for $\textrm{NP}^{\textrm{PP}}$, and that no algorithm can achieve an approximation factor (in terms of $k^\star$) of $d^{1-\alpha}$, $\alpha > 0$, unless $\ptime = \textrm{NP}$. Since then,~\cite{Arenas_Barcelo_Romero_Subercaseaux_2022} showed that even for the restricted class of decision trees, usually considered the interpretable, smallest $\delta$-SRs cannot be computed unless $\ptime = \textrm{NP}$, and furthermore,~\cite{Kozachinskiy_2023} proved that the approximation task is also hard for decision trees. Notably, none of these hardness results rely on the distribution $\D$ being complicated, as they were proved taking $\D = \textrm{Uniform}(\{0, 1\}^d)$. Let us formally define the computational problem at hand, assuming the uniform distribution.



 \csproblem{Uniform-Min-$\delta$-SR$(\mathcal{C})$}{a model $\M \in \mathcal{C}$, an instance $\vx$, a value $\delta \in [0, 1]$, and an integer $k$.}{\textsc{\bf Yes} if $k^\star(\M, \vx, \delta) \leq k$, and \textsc{\bf No} otherwise.}
 
Unfortunately, it turns out that even Uniform-Min-$\delta$-SR$(\textsc{Linear})$ is hard, as the amount
\[
    \Pr_{\vz \sim  \D(\vy)}\Big[\Lin(\vz) = 1\Big]
\]
cannot be computed in polynomial time unless $\ptime=\# \ptime$~\cite{NEURIPS2020_b1adda14}. 

\begin{proposition}
    Uniform-Min-$\delta$-SR$(\textsc{Linear})$ cannot be solved in polynomial time unless $\ptime = \# \ptime$.
\end{proposition}
\begin{proof}
    The proof is a simple modification of~\cite[Lemma 28]{NEURIPS2020_b1adda14}; let $(s_1, \ldots, s_n, T) \in \mathbb{N}^{n+1}$ be an instance of the $\# \ptime$-complete problem $\#\Knapsack$, that consists on counting the number of sets $S \subseteq \{s_1, \ldots, s_n\}$ such that $\sum_{s \in S}s \leq T$.  
    We can assume that $\sum_{i=1}^n s_i > T$, as otherwise the $\# \Knapsack$ instance is trivial.
    Then, let $\Lin$ be a linear model with weights $w_i = s_i$, and threshold $t = T+1$.
    Now, consider the problem of deciding whether $\#\Knapsack(s_1, \ldots, s_n, T) \geq k$, which cannot be solved in polynomial time unless $\ptime = \# \ptime$.
    Let $\vx = (1, 1, \ldots, 1)$, and $\delta = \frac{k}{2^{n}}$. We claim that $(\Lin, \vx, \delta, 0)$ is a Yes-instance for Uniform-Min-$\delta$-SR$(\textsc{Linear})$ if and only if $\#Knapsack(s_1, \ldots, s_n, T) \geq k$. 
    First, note that $\Lin(\vx) = 1$, since $\sum_{i=1}^n w_i x_i = \sum_{i=1}^n s_i \geq T+1 = t$. 
    For every set $S \subseteq \{s_1, \ldots, s_n\}$ such that $\sum_{s \in S}s \leq T$, its complement $\overline{S} := \{s_1, \ldots, s_n\} \setminus S$ holds $\sum_{s \in \overline{S}}s > T$, and as all values are integers, this implies as well
    \[
        \sum_{s \in \overline{S}} s \geq T+1 = t.
    \]
    To each such set $\overline{S}$, we associate the instance $\vz(\overline{S})$ defined as
    \[
        z(\overline{S})_i = \begin{cases}
            1 & \text{if } s_i \in \overline{S}\\
            0 & \text{otherwise}.
        \end{cases}
    \]
    Now note that 
    \[
        \Lin(\vz(\overline{S})) = \begin{cases}
            1 & \text{if } \sum_{s_i \in \overline{S}} w_i \geq T+1\\
            0 & \text{otherwise} \end{cases} = 1,
    \]
    and thus there is a bijection between the sets $S$ whose sum is at most $T$ and the instances $\vz(\overline{S})$ such that $\Lin(\vz(\overline{S})) = 1 = \Lin(\vx)$.
    To conclude, simply note that the previous bijection implies
    \[
        \Pr_{\vz \sim U(\{0, 1\}^n)}\Big[\Lin(\vz) = 1\Big] = \frac{\#\Knapsack(s_1, \ldots, s_n, T)}{2^n},
    \] 
    and thus $\vy$ has probability at least $\delta$ if and only if $\#\Knapsack(s_1, \ldots, s_n, T) \geq k$.
    

%     First, recall that the problem of counting the number of ``positive completions'' of a partial instance $\vy$ is $\# \ptime$-hard for linear models~\cite{NEURIPS2020_b1adda14}; that is, given a partial instance $\vy$, a linear model $\Lin$ and a positive integer $K$, it is $\mathrm{PP}$-hard to determine whether there are at least $K$ instances $\vz \in \comp(\vy)$ such that $\Lin(\vz) = 1$. That result follows from the hardness of $\sharpK$.
%    Moreover, in the proof of~\cite{NEURIPS2020_b1adda14} all weights are positive $\delta = \frac{V}{2^{n - |\vy|_\bot}}.$ 
\end{proof}

We will consider the problem of approximating the minimum size $\delta$-SRs for linear models, first under the uniform distribution, and then under product distributions. To do this, let us consider two senses in which one can approximate $\delta$-SRs.
\begin{itemize}
    \item \textbf{Approximation in terms of $\delta$}: Given a linear model $\Lin$, an instance $\vx$, and a value $\delta$, we want to compute a $\delta'$-SR of size $k^\star(\Lin, \vx, \delta')$ such that $\delta'$ is close to $\delta$. Intuitively, this corresponds to the idea of stakeholders not caring about the exact value of $\delta$; e.g., $90\%$ of completions of $\vy$ agree with $\vx$ has roughly the same human implications as $89.9997\%$.
    \item \textbf{Approximation in terms of $k^\star$}: Given a linear model $\Lin$, an instance $\vx$, and a value $\delta$, we want to compute a $\delta$-SR of size $k$ such that $k$ is not much bigger than $k^\star(\Lin, \vx, \delta)$. Intuitively, even though stakeholders want \emph{small} explanations, it is not required to find the smallest possible explanation.
\end{itemize}

We can now state an initial result.

\begin{theorem}\label{thm:delta-approximation}
    The Uniform-Min-$\delta$-SR$(\textsc{Linear})$ problem admits an FPRAS with respect to $\delta$. In particular, there exists an algorithm that computes a minimum $\delta'$-SR for some $\delta' \in [\delta, \delta + \epsilon]$, and succeeds with probability at least $1-\gamma$, running in time $\tilde{O}\left(   \frac{d^2}{\epsilon^2\gamma^2}\right)$.
\end{theorem}

% \begin{theorem}\label{thm:k-approximation}
%     The Uniform-Min-$\delta$-SR$(\textsc{Linear})$ problem can be approximated over $k^\star$ with additive error $1$ and probability of success $1-\gamma$ in time $\tilde{O}\left( \frac{d^2}{\epsilon^2\gamma^2}\right)$.
% \end{theorem}

In order to prove it we will need two basic ideas: first, the fact that we can estimate the probabilities of linear models accepting a partial instance through sampling, and second, that under the uniform distribution it is easy to decide which features ought to be part of small explanations.

\subsection{Estimating the Probability of Acceptance}
 The  hardness of computing
$\Pr_{\vz  \in \D(\vy)}[\M(\vz) = 1]$ is about computing it to arbitrarily high precision, i.e., with an additive error within $O(2^{-d})$. However, computing a less precise estimation of $\Pr_{\vz \in \D(\vy)}[\M(\vz) = 1]$ is simple, as the next fact (which is a direct consequence of Hoeffding's inequality)  states.

\begin{fact}\label{fact:hoeffding}
    Let $f$ be an arbitrary boolean function on $n$ variables. Let $M$ be any integer,
    and sample $M$ inputs $x_1, \ldots, x_M$ uniformly at random from $\{0, 1\}^n$. Then 
    \(
        \hat{\mu} := \frac{\sum_{i=1}^M [f(x_i) = 1]}{M}
    \)
    is an unbiased estimator for 
    \(
        \mu := \Pr_{x \in \{0, 1\}^n}[f(x) = 1],
    \)
    and 
    \[
    \Pr[\left|\hat{\mu} - \mu \right| \leq t] \geq 1 - 2e^{-2t^2 M},
    \]
    which is at least $1 - \gamma$ for $M = \frac{1}{2t^2} \log(2/\gamma)$.
\end{fact}

 \paragraph{Smoothed Explanations} As a consequence of the previous idea, although a minimum $\delta$-SR might be hard to compute, this crucially depends on the value of $\delta$. In the spirit of smoothed analysis, we define the computation of a min-$(\delta, \epsilon)$-SR as follows: first, a value $\delta^\star$ is chosen uniformly at random from $[\delta-\epsilon, \delta+\epsilon]$, and then a min-$\delta^\star$-SR is computed. Intuitively, the idea is that as $\delta^\star$ is chosen at random, it will be unlikely that a value that makes the computation hard is chosen. 

 We will prove the following proposition:

\begin{proposition}
    \label{prop:smoothed-explanation}
    Given a linear model $\Lin$ and an input $\vx$, we can compute a $(\delta, \epsilon)$-SR successfully with probability $1 - \gamma$ in time polynomial in $d$, $1/\epsilon$ and $1/\gamma$. In particular, in time $\tilde{O}\left( \frac{d^2}{\epsilon^2\gamma^2}\right)$.
\end{proposition}

Note that~\Cref{prop:smoothed-explanation} immediately gives us~\Cref{thm:delta-approximation}, as we can set $\delta' =  \delta+\epsilon/2$ and $\epsilon' = \epsilon/2$, which means with probability $1 - \gamma$ we will obtain a $(\delta', \epsilon')$-SR, whose probability guarantee is in 
\[
  [\delta' - \epsilon', \delta' + \epsilon'] =  [\delta, \delta + \epsilon].
\]

Before proving~\Cref{prop:smoothed-explanation}, we need to prove a lemma concerning the easiness of selecting the features of the desired explanation.

\subsection{Feature Selection}

Even if we were granted an oracle computing the probabilities
\(
    \Pr_{\vz  \in \D(\vy)}[\M(\vz) = 1]
\), that would not be necessarily enough to efficiently compute the minimum $\delta$-SR. Indeed, for decision trees, the counting problem can be easily solved in polynomial time~\cite{NEURIPS2020_b1adda14}, and yet the computation of $\delta$-SRs of minimum size is hard, even to approximate~\cite{Arenas_Barcelo_Romero_Subercaseaux_2022,Kozachinskiy_2023}.
Intuitively, the problem for decision trees is that, even if we were told that the minimum $\delta$-SR has exactly $k$ features, it is not obvious how to search for it better than enumerating all $\binom{d}{k}$ subsets. The case of linear models, however, is different, at least under the uniform distribution. In this case, every feature $i$ that is not part of the explanation will take value $0$ or $1$ independently with probability $\nicefrac{1}{2}$, and contribute to the classification according to its weight $w_i$. In other words, we can sort the features according to their weights (with some care about signs), and select them greedily to build a small $\delta$-SR. A proof for the deterministic case ($\delta = 1$) was already given in~\cite{NEURIPS2020_b1adda14} and sketched earlier on by~\cite{ExplainingNaiveBayes}.

\begin{definition}
    Given a linear model $\Lin = (\vw, t)$, and an instance $\vx$, both having dimension $d$, we define the \emph{score} of feature $i \in [d]$ as 
    \[
        s_i := w_i \cdot (2x_i - 1) \cdot (2\left(\Lin(\vx) - 1\right)).    
    \]
\end{definition}

In other words, the sign of $s_i$ is $+1$ if the feature is ``helping'' the classification, and $-1$ if it is ``hurting'' it. The magnitude of $s_i$ is proportional to the weight of the feature $i$.
For the uniform distribution, we can prove the following lemma that basically states that, if we are promised a $\delta$-SR of size $k$ exists, then one can be found by taking the top-$k$ features of maximum score $s_i$.

\begin{lemma}\label{lemma:greedy}
Given a linear model $\Lin$, an instance $\vx$,  a value $\delta \in (0, 1]$, and an integer $k$, then there exists a $\delta$-SR for $\vx$ under $\Lin$ of size $k$ if and only if the partial instance defined only on the $k$ features of $\vx$ with maximum score is a $\delta$-SR for $\vx$ under $\Lin$.
\end{lemma}

\todo[inline]{Bernardo: The proof of this is trivial but annoying, it suffices to do an exchange argument. I will write it down later.}

With this lemma in hand, we can now prove~\Cref{prop:smoothed-explanation}.

\input{proof-of-thm1.tex}


% Let us restate and prove our other theorem.

% \begin{theorem}
%     The Uniform-Min-$\delta$-SR$(\textsc{Linear})$ problem can be approximated over $k^\star$ with additive error $1$ and probability of success $1-\gamma$ in time $\tilde{O}\left( \frac{d^2}{\epsilon^2\gamma^2}\right)$.
% \end{theorem}
% \begin{proof}[Proof sketch]
%     Sort the features according to \emph{score} again, then we try to find the minimum $k$ such that the top-$k$ most important features give a probability of $\delta$. In particular, we will accept the first $k$ such that the empirical probability is above $\delta + (1-\delta)/2d$. The chances of the ``true'' probability being above $\delta$ will naturally be good, the problem is to guarantee that we won't overshoot by much. The main insight here is that, if $k^*$ is the true optimal answer (thus having probability at least $\delta$), then $k^*+1$ should always have the desired probability; note that the remaining probability is $1-\delta$, and there are $d-{k^*}$ features remaining, so the next most important one must yield at least a gain of 
%     $\frac{1-\delta}{d - k^*} \geq \frac{1 - \delta}{d}$. So by sampling with an error of $\frac{1-\delta}{2d}$ we will recognize this, which by using the fact requires 
% $
%     \tilde{O}\left(\left(\frac{1-\delta}{2d}\right)^2\right)
% $ many samples to ensure with high probability. Note that the problematic case is now when $\delta \approx 1$. 
% \end{proof}