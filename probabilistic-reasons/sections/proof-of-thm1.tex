
\begin{algorithm}[tb]
	\caption{LinearMonteCarloExplainer}
	\label{alg:algorithm}
	\textbf{Input}: A linear model $\Lin$, an instance $\vx$, and $\delta \in (0, 1)$.\\
	\textbf{Parameters}:  $\varepsilon \in (0, 1)$,  $\gamma \in (0, 1)$.\\
	\textbf{Output}: A value $\delta^\star \in [\delta-\varepsilon, \delta+\varepsilon]$ together with a minimum $\delta^\star$-SR explanation for $\vx$.\\
	\begin{algorithmic}[1] %[1] enables line numbers
	\STATE $\delta^\star \gets$ uniformly random sample from $[\delta-\varepsilon, \delta + \varepsilon]$ \label{line:delta}
	\FOR{$i \in \{1, \ldots, d\}$}
		\STATE $s_i \gets  w_i \cdot (2x_i - 1) \cdot (2\Lin(\vx) - 1)$
	\ENDFOR
	
	\FOR{$k \in\{0,1 \ldots, d\}$}
		\STATE  let $\vy^{(k)} \subseteq \vx$ be the partial instance defined only in the top $k$ features with maximum score $s_i$.\label{line:greedy}
	\ENDFOR
	% \STATE Let $\ell$ be the max value such that $\vec{\mathcal{F}}[\ell] = (\lambda, s_\lambda)$ has $s_\lambda > 0$.
	\STATE $M \gets (\log^2{d})/(2\varepsilon^2 \gamma^2) \log(2 \log d / \gamma)$\label{line:mdef}
	\STATE $\textrm{LB} \gets 0$, $\textrm{UB} \gets d$, and $\textsc{steps} \gets 0$

	\WHILE{$\textrm{LB} \neq \textrm{UB}$ and $\textsc{steps} \leq \log d$}\label{line:while}
		\STATE $\textsc{steps} \gets \textsc{steps} + 1$\label{line:steps}
	% \COMMENT{Binary Search}
		\STATE $m \gets \left(\textrm{LB} + \textrm{UB} \right)/2$
		\STATE $\widehat{v}_m \gets \textsc{MonteCarloEstimation}(\Lin, \vy^{(m)}, \vx, M)$ \label{line:montecarlo}
		\IF {$\widehat{v}_m \geq \delta^\star$}
			\STATE $\textrm{UB} \gets m$
		\ELSE
			\STATE $\textrm{LB} \gets (m+1)$
		\ENDIF
	\ENDWHILE\label{line:endwhile}
	\STATE $k^\star \gets \textrm{LB}$ (or equivalently, $\textrm{UB}$)
	\STATE \RETURN $(\delta^\star, \vy^{(k^\star)})$
	\end{algorithmic}
	\end{algorithm}


\begin{algorithm}[tb]
	\caption{MonteCarloEstimation}	\label{alg:montecarlo}
	\textbf{Input}: A linear model $\Lin$, a partial instance $\vy$, an instance $\vx$, and a number of samples $M \in \mathbb{N}$.\\
	\textbf{Output}: An estimate~$\widehat{v}$~of~$\Pr_{\vz \sim \U(\vy)}[\Lin(\vz)=\Lin(\vx)]$.\\
	\begin{algorithmic}[1]
	\STATE $\widehat{v} \gets 0$
	\FOR{$i = 1$ to $M$}
		\STATE Sample $\vz \sim \U(\vy)$
		\IF {$\Lin(\vz) = \Lin(\vx)$}
			\STATE $\widehat{v} \gets \widehat{v} + 1$
		\ENDIF \ENDFOR
	\STATE \RETURN $\widehat{v}/M$.
\end{algorithmic}
\end{algorithm}


	

\begin{proof}[Proof of~\Cref{prop:smoothed-explanation}]
We use~\Cref{alg:algorithm}. Let us define the partial instances $\vy^{(0)}, \vy^{(1)} \ldots, \vy^{(d)}$ so that $\vy^{(k)} \subseteq \vx$ is the partial instance defined only in the $k$ features with maximum score (line~\ref{line:greedy}). %and let $\ell$ be the first index such the $\ell$-th feature when sorted decreasingly by score has a negative score (line 5).
We then define a sequence of values
  $v_k$ as
	\[
		v_k := \Pr_{\vz \sim \U(\vy^{(k)})}[\Lin(\vz) = \Lin(\vx)],
	\]
	and note that due to~\Cref{lemma:greedy}, the sequence $v_0, \ldots, v_d$ is non-decreasing.
Let $M = \frac{\log^2{d}}{2\varepsilon^2 \gamma^2} \log(2 \log d / \gamma)$, as in line~\ref{line:mdef}, and let us define random variables $\widetilde{v_k}$ as follows: if~\Cref{alg:algorithm} enters line~\ref{line:montecarlo} with $m=k$, then $\widetilde{v_k}$ is the output of~\Cref{alg:montecarlo} (i.e., $\widehat{v}_k(M)$), and otherwise $\widetilde{v_k} = v_k$.  
% Because of~\Cref{lemma:monotonicity}, we have that the sequence of values $v_1, \ldots, v_\ell$ is non-decreasing.
We use binary search (lines~\ref{line:while}-\ref{line:endwhile}), to find $k^\star$, the smallest $k$ such that
\[
	\widetilde{v_k} \geq \delta^\star,
\]
and our goal is to show that with good probability $k^\star$ is also the smallest $k$ such that $v_k \geq \delta^\star$, which would imply the correctness of the algorithm by~\Cref{lemma:greedy}.
Note, however, that even though the sequence $v_0, \ldots, v_d$ is non-decreasing (\Cref{lemma:greedy}), the estimated values $\widehat{v_k}$ are not necessarily so.
Let $S$ be a random variable corresponding to the set of values $k$ such that~\Cref{alg:algorithm} enters line~\ref{line:montecarlo} with $m=k$, and note that if for every $k$ in $S$ it happens that the events 
\[
	A_k := \left(v_k \geq \delta^\star \right) \text{ and }  B_k := \left(\widetilde{v_k} \geq \delta^\star\right)
\]
are equivalent (i.e., either both occur or neither occurs), then the algorithm will succeed, as that would indeed imply that $k^\star$ is the smallest $k$ such that $v_k \geq \delta^\star$. 

Then, for $k \in [d]$, define events $E_k$ and $F_k$ as follows:
\begin{align*}
	E_k &:= |\delta^\star - v_k| \geq \frac{\varepsilon \gamma}{\log d},\\
	F_k &:= |\widetilde{v_k} - v_k| \leq \frac{\varepsilon \gamma}{\log d}.
\end{align*}
We claim that if both $E_k$ and $F_k$ hold for some $k$, then $A_k$ and $B_k$ are equivalent events for that $k$. Indeed,
\begin{align*}
	A_k &\iff v_k \geq \delta^\star\\
		&\iff v_k \geq \delta^\star + \frac{\varepsilon \gamma}{\log d} \tag{by $E_k$}\\
		&\iff v_k - \frac{\varepsilon \gamma}{\log d} \geq \delta^\star\\
		&\iff \widetilde{v_k} \geq \delta^\star \tag{by $F_k$}\\
		&\iff B_k.
\end{align*}

Thus, if we show that $E_k$ and $F_k$ hold with good probability for every $k \in S$, we can conclude the theorem.
First, note that because of the condition on the variable $\textsc{steps}$ (lines~\ref{line:while}, \ref{line:steps}) we have $|S| \leq \log d$, allowing us to do a binary search in case the desired events $E_k$ and $F_k$ hold, and preventing the algorithm from looping otherwise; this way the runtime is bounded not only on expectation but deterministically. \footnote{For simplicity, we will say $|S| \leq \log d$, even though the exact bound for a binary search is $|S| \leq \lfloor \log d  + 1 \rfloor$. This choice naturally has no impact on the asymptotic analysis of the algorithm.} Then, note that for any $k$ we have
\begin{align*}
	\Pr\left[ \, \overline{F_k} \,\right] \leq \Pr\left[ \, \overline{F_k} \mid k \in S \right] &= \Pr\left[|\widehat{v_k}(M) - v_k| > \frac{\varepsilon \gamma}{\log d}\right]\\
	&\leq \frac{\gamma}{\log d}. \tag{by~\Cref{fact:hoeffding}}
\end{align*}
Because $S$ itself is a random variable, whose size is also a random variable, we need to be careful before applying a union bound or any related tricks. Let us refer to the elements of $S$ as $\{s_1, \ldots, s_\ell\}$, and let us call $F(i)$, for $i \in [\log d]$,\footnote{Throughout this proof, we use notation $[\alpha]$, for $\alpha \in \mathbb{R}^{> 0}$, to denote the set $\{0, \ldots, \lceil \alpha \rceil\}$.} to the event $F_{s_i}$ if $i \leq \ell$, and to the sample space $\Omega$ (i.e., the event that always happens) otherwise. Then, we claim that for any $0 \leq i \neq j \leq \log d$, we have 
\begin{equation}\label{eq:quasi-independence}
	\Pr[F(i) \cap F(j)] = \Pr[F(i)] \cdot \Pr[F(j)],
\end{equation}
as either $\max \{i, j\} \leq \ell$, in which case the claim holds by independence (since both events depend only on disjoint sets of independent random samples), or the claim holds trivially since $\Pr[F(i)] = 1$ for $i > \ell$.
Therefore, we have 
\begin{align*}
\Pr\left[\bigcap_{k \in S} F_k\right] &= \Pr\left[F(0) \cap F(1) \cap \cdots \cap F(\log d)\right]\\
&= \prod_{i \in [\log d]} \Pr[F(i)]\tag{by~\Cref{eq:quasi-independence}}\\
&\geq \left(1 - \frac{\gamma}{\log d}\right)^{\log d} \geq 1 - \gamma.
\end{align*}
% Note that different events $\overline{F_i}, \overline{F_j}$ are not necessarily independent, as they depend on $S$ which is itself a random variable. However, they depend only on whether $i$ or $j$ are in $S$. Concretely,
% \[ 
% \Pr[\overline{F_i} \mid \overline{F_j} \text{ and } i \in S] = \Pr[\overline{F_i} \mid  i \in S] \leq \frac{\gamma}{\log d}.
% \]
% It is now tempting to make a union bound over the $\log d$ elements of $S$ and claim simply that:
% \begin{align*}
% 	\Pr\left[\bigcap_{k \in S} F_k\right] &= 1 - \Pr\left[\bigcup_{k \in S} \overline{F_k}\right]\\
% 	&\geq 1 - \sum_{k \in S} \Pr\left[\,\overline{F_k} \mid k \in S\right]\\
% 	&\geq 1 - \sum_{k \in S} \frac{\gamma}{\log d}\\
% 	&= 1 - \gamma.
% \end{align*}
% However, $S$ itself is a random variable, and $F_k$ is not independent of $S$. We need, therefore, to be more careful. In this case, we can get around this by denoting $s_1, \ldots, s_{|S|}$ the elements of $S$ (in e.g., increasing order), which are random variables (and furthermore, note that the number of variables $s_i$ is itself a random variable).
% Observe that $F_{s_i}$ is independent of $F_{s_j}$ for $i \neq j$,  

% Note that, even though $F_k$ depends on $S$, it actually ``only depends'' on whether $k \in S$. Concretely, for any $S' \subseteq [d]$ it holds that
% \begin{multline}\label{eq:1}
% 	\Pr[\, \overline{F_k} \,] \leq \Pr[\, \overline{F_k} \mid k \in S \,] = \Pr[\, \overline{F_k} \mid k \in S \text { and } S = S'] \\
% 	\leq \frac{\gamma}{\log d}.
% \end{multline}

% Then, note that 
% \begin{align*}
% 	\Pr\left[\bigcup_{k \in S} \overline{F_k}\right] &= \sum_{S' \subseteq [d]} \Pr\left[S = S' \text { and } \bigcup_{k \in S'} \overline{F_k}\right]\\
% 	&= \sum_{S' \subseteq [d]} \Pr\left[\bigcup_{k \in S'} \left(\overline{F_k} \text { and } S = S'\right)\right]\\
% 	&\leq \sum_{S' \subseteq [d]} \sum_{k \in S'} \Pr\left[\overline{F_k} \text { and } S = S'\right] \tag{Union bound}\\
% 	&= \sum_{S' \subseteq [d]} \sum_{k \in S'} \Pr\left[S = S'\right] \Pr\left[\overline{F_k} \mid S = S'\right]\\
% 	&\leq \sum_{S' \subseteq [d]} \Pr\left[S = S'\right] \sum_{k \in S'} \frac{\gamma}{\log d} \tag{\Cref{eq:1}}\\
% 	&=\sum_{\substack{S' \subseteq [d]\\ |S'| \leq \log d}} \Pr\left[S = S'\right] \sum_{k \in S'} \frac{\gamma}{\log d}\\
% 	&\leq \sum_{\substack{S' \subseteq [d]\\ |S'| \leq \log d}} \Pr\left[S = S'\right] \cdot \gamma\\
% 	&= \gamma.
% \end{align*}

% We thus have concluded that $\Pr\left[\bigcap_{k \in S} F_k\right] \geq 1 - \gamma$.
We now argue that the event $\bigcap_{k \in S} E_k$ happens with good probability.
To see that, note first that for every $k \in [d]$, line~\ref{line:delta} implies
\[
	\Pr[\overline{E_k}] = \Pr\left[\delta^\star \in \left[v_k \pm \frac{\varepsilon \gamma}{\log d}\right]\right] \leq \frac{\frac{2\varepsilon \gamma}{\log d}}{2\varepsilon} = \frac{\gamma}{\log d}.
\]
Once again, we need to be careful as the events $E_k$ are not independent of $S$, nor between them this time.
Using the law of total probabilities, we have 
\begin{align*}
	\Pr\left[\bigcap_{k \in S} E_k\right] &= \sum_{S' \subseteq [d]} \Pr\left[S = S'  \mid \bigcap_{k \in S'} E_k \right] \Pr\left[\bigcap_{k \in S'} E_k\right]\\
	&= \sum_{\substack{S' \subseteq [d]\\ |S'| \leq \log d}} \Pr\left[S = S' \mid \bigcap_{k \in S'} E_k\right] \Pr\left[\bigcap_{k \in S'} E_k\right],
\end{align*}
where we can now effectively use the union bound to say that for any fixed $S'$ with $|S'| \leq \log d$ we have
\[ 
	\Pr\left[\bigcap_{k \in S'} E_k \right] \geq 1 - \gamma.
\]
Therefore, we conclude that
\begin{equation}
	\Pr\left[\bigcap_{k \in S} E_k\right] =  \sum_{\substack{S' \subseteq [d]\\ |S'| \leq \log d}} \Pr\left[S = S'\mid \bigcap_{k \in S'} E_k\right] \Pr\left[\bigcap_{k \in S'} E_k\right] \geq (1-\gamma) \sum_{\substack{S' \subseteq [d]\\ |S'| \leq \log d}} \Pr\left[S = S' \mid \bigcap_{k \in S'} E_k\right]. \label{eq:align}
\end{equation}
It is not obvious, however, whether the sum on the right-hand side of~\Cref{eq:align}~equals $1$; we will, however, argue that it is at least $1-\gamma$. 
Recall that $\Pr[F_k | k \not\in S] \geq \Pr[F_k | k \in S]$ for any $k$, from where it follows that for every set $S'$ of size at most $\log d$ we have 
\[ 
	\Pr\left[\bigcap_{k \in S'} F_k \right] \geq \Pr\left[\bigcap_{k \in S} F_k \right].	
\]
Then, note that for any index $k \in [d]$, the event $F_k$ is conditionally independent of all events $E_{j}$, with $j \in [d]$ given the event $k \in S$. That is,
\[
	\Pr\left[F_k | k \in S\right] = \Pr\left[F_k | E_{j}, k \in S\right], \quad \text{for any } j \in [d].
\]
We thus deduce that for any fixed $S'$ with $|S'| \leq \log d$ we have
\begin{align}\label{eq:2}
	\begin{split}
	\Pr\left[\bigcap_{k \in S'} F_k \mid \bigcap_{k \in S'} E_k\right] &    \geq  \Pr\left[\bigcap_{k \in S'} F_k \mid \bigcap_{k \in S'} E_k \text{ and  } \bigcap_{k \in S'} {k \in S}\right] \\
	&= \Pr\left[\bigcap_{k \in S'} F_k \mid \bigcap_{k \in S'} {k \in S}\right] \geq \Pr\left[\bigcap_{k \in S} F_k\right] \geq (1-\gamma).
	\end{split}
\end{align}
Then, our key observation is that there is a single value $S^\star \subseteq [d]$, with $|S^\star| \leq \log d$, that the binary search can take if we condition on all the events $E_k$ and $F_k$ happening, since in that case events $A_k$ and $B_k$ coincide. In other words, there exists $S^\star$, with $S^\star \subseteq [d]$ and $|S^\star| \leq \log d$, such that
\begin{equation}\label{eq:prob1}
\Pr\left[S = S^\star \mid \bigcap_{k \in S^\star} E_k \cap \bigcap_{k \in S^\star} F_k\right] = 1.
\end{equation}
We can then argue as follows:
\begin{align*}
	\Pr\left[\bigcap_{k \in S} E_k\right] &\geq (1-\gamma) \sum_{\substack{S' \subseteq [d]\\ |S'| \leq \log d}} \Pr\left[S = S' \mid \bigcap_{k \in S'} E_k\right]\tag{by~\Cref{eq:align}}\\
	&\geq (1-\gamma) \Pr\left[S = S^\star  \mid \bigcap_{k \in S^\star} E_k \right]\\
	&\geq (1-\gamma) \Pr\left[S = S^\star \text{ and } \bigcap_{k \in S^\star} F_k \mid \bigcap_{k \in S^\star} E_k \right]\\
	&= (1-\gamma) \Pr\left[S = S^\star \mid \bigcap_{k \in S^\star} E_k \cap \bigcap_{k \in S^\star} F_k\right] \cdot \Pr\left[\bigcap_{k \in S^\star} F_k \mid \bigcap_{k \in S^\star} E_k\right]\tag{Bayes' rule}\\
	&\geq (1-\gamma)^2. \tag{by~\Cref{eq:2,eq:prob1}.}
\end{align*}


Therefore, the algorithm will succeed with probability at least 
\[ 
	\Pr\left[\bigcap_{k \in S} E_k\right] \cdot \Pr\left[\bigcap_{k \in S} F_k\right] \geq (1-\gamma)^3 \geq 1-3\gamma.
\]
The runtime is simply
$O(\log d \cdot M \cdot d  )$; as (i) the binary search performs $O(\log d)$ steps; (ii) each of the binary search steps requires $M$ samples, and (iii) each sample requires evaluating the model $\Lin$ and thus takes time $O(d)$. Naturally, running the algorithm with $\gamma' = 1/3 \cdot \gamma$ will yield a success probability of $1-\gamma$ without changing the asymptotic runtime, and thus we conclude the proof.


% We thus want to lower bound the probability of some decently large value $\Delta$ such that $|\delta^\star - v_k| \geq \Delta$ for every $k \in [d]$; that way, it will be enough to estimate the values $v_k$ up to an additive error of $\Delta/2$. Given that there are $d$ values $v_
% k$, and each of them forbids a segment of size $2\Delta$ for $\delta^\star$, then the probability of choosing a value of $\delta^\star$ such that $|\delta^\star - v_k| \geq \Delta$ for every $k \in [d]$ is at least
% \[
% 	1 - \frac{d\Delta}{\varepsilon}.
% \]
% As we want our algorithm to succeed with probability $1 - \gamma$, that requires 
% \(
% \frac{d\Delta}{\varepsilon} \leq \gamma
% \)
% and thus
% \[
% 	\Delta \leq \frac{\varepsilon \gamma}{d}.
% \]
% Thus we need to sample with an additive error smaller than
% \(
% \frac{\varepsilon}{2d \gamma},
% \)
% which requires the number of samples $M$ to be such that
% \[
% 	\frac{\log M}{\sqrt{M}} \leq \frac{\varepsilon\gamma}{2d}.
% \] 
% %%Considering that
% %%\[
% %%	\frac{\log M}{\sqrt{M}} \leq \frac{\log M}{\log M \sqrt[2+\gamma]{M}} = \frac{1}{\sqrt[2+\gamma]{M}},
% %%\]
% %Let us try to simplify things by considering establishing
% %\[
% %	\frac{1}{\sqrt{M'}} \leq \frac{\varepsilon\gamma}{2d}, 
% %\]
% %or equivalently,
% %\[
% %	M \geq \left( \frac{2d}{\varepsilon\gamma} \right)^2.
% %\]
% Now we pose
% \[
% 	M =	9\cdot \left( \frac{2d}{\varepsilon\gamma} \right)^2\log\left(\frac{2d}{\varepsilon\gamma}\right)^2,
% \]
% so that $\log M \leq 3\log\left(\frac{2d}{\varepsilon\gamma}\right)$, and thus
% \[
% 	\frac{\log M}{\sqrt{M}} \leq \frac{3\log(2/(\varepsilon\gamma))}{3\left( \frac{2d}{\varepsilon\gamma} \right)\log\left(\frac{2d}{\varepsilon\gamma}\right)} = \frac{\varepsilon\gamma}{2d},
% \]
% as desired.

% Thus, the algorithm above, which requires a sampling process at most $\log d$ times, using binary search to find the min, can be carried out with probability of success at least $1-\gamma$. in time 
% \[
% O(\log d \cdot M) = \tilde{O}\left(\left(\frac{d}{\varepsilon\gamma}\right)^2\right).
% \]


% \begin{figure}
% 	\centering
% 	\scalebox{0.8}{
% 	\begin{tikzpicture}
% 		\node[] (beg) at (0, 0) {};
% 		\node[] (end) at (11, 0) {};
% 				\node[] (a) at (10, 0.5) {\small $1$};
% 		\node[] (amark) at (10, 0) {\small $\mid$};
% 		\node[] (b) at (5, 0.5) {\small $\nicefrac{1}{2}$};
		
% 		\node[] (b) at (0, 0.5) {\small $0$};
		
% 		\node[] (bmark) at (5, 0) {\small $\mid$};
		
% 		\node[] (leftPar) at (5.5, 0) {\textcolor{blue}{$\Big[$}};
% 		\node[] (rightPar) at (9.5, 0) {\textcolor{blue}{$\Big]$}};
		
% 		\draw[-, blue, dashed, very thick]  (leftPar) -- (rightPar);
		
% 		\draw[|-, very thick] (beg) -- (leftPar);
% 			\draw[->, very thick] (rightPar) -- (end);

		
% 		\node[] (left) at (5.5, -0.7) {\textcolor{blue}{\scriptsize $\delta-\varepsilon$}};
% 		\node[] (right) at (9.5, -0.7) {\textcolor{blue}{\scriptsize $\delta+\varepsilon$}};
		
% 		\node[] (delta) at (7.5, -0.5) {\textcolor{blue}{$\delta$}};
		
% 		\node[] (dmark) at (7.5, 0) {\textcolor{blue}{\small $\mid$}};
		
		
% 		\node[] (v1) at (8.8, 0) {\textcolor{purple}{\small $\mid$}};
% 		\node[] (v1) at (8.8, -0.4) {\textcolor{purple}{\scriptsize $v_1$}};
		
% 		\node[] (v2) at (6.6, 0) {\textcolor{purple}{\small $\mid$}};
% 		\node[] (v2) at (6.6, -0.4) {\textcolor{purple}{\scriptsize $v_2$}};
		
% 		\node[] (v4) at (3.6, 0) {\textcolor{purple}{\small $\mid$}};
% 		\node[] (v4) at (3.6, -0.4) {\textcolor{purple}{\scriptsize $v_3$}};
		
% 		\node[] (v3) at (1.2, 0) {\textcolor{purple}{\small $\mid$}};
% 		\node[] (v3) at (1.2, -0.4) {\textcolor{purple}{\scriptsize $v_4$}};
		
% 		\draw[rectangle, fill=purple, line width=0pt] (6.3, -0.2) -- (6.3, 0.2) -- (6.9, 0.2) -- (6.9, -0.2) -- (6.3, -0.2);
		
% 		\draw[rectangle, fill=purple, line width=0pt] (8.5, -0.2) -- (8.5, 0.2) -- (9.1, 0.2) -- (9.1, -0.2) -- (8.5, -0.2);
		
		
% 		\node at (6.6, 0.65) {$\Delta$};
% 		\node at (8.8, 0.65) {$\Delta$};
% 		\draw[decorate, decoration={brace, amplitude=3pt}, thick] (6.3, 0.28) -- (6.9, 0.28);
		
% 			\draw[decorate, decoration={brace, amplitude=3pt}, thick] (8.5, 0.28) -- (9.1, 0.28);
		
% 		\node[] (deltastar) at (8.1, -1.2) {\large \textcolor{violet}{$\delta^\star$}};
% 		\draw[-, , violet, thick] (8.0, -0.9) -- (8.0, 0.9);

 
% 	\end{tikzpicture}
% 	}
% 	\caption{Illustration of the proof, using $d=4, \delta = \nicefrac{3}{4}$ as an example. In this case, the probability of failure is $\frac{2\Delta}{\varepsilon}.$}
% \end{figure}

\end{proof}