\section{Probabilistic Sufficient Reasons}

The main idea of probabilistic sufficient reasons is to relax the condition \emph{``all completions of the explanation $\vy$ have the same class as $\vx$''} to \emph{``a random completion of $\vy$ has the same class as $\vx$ with high probability''}. To define this formally let us start by assuming an instance distribution $\D$, meaning that an input $\vx$ has probability $\D(\vx)$ of arising. For example, if one were to deploy an algorithm for handwritten-digit classification over $28 \times 28$ binary images (as in a binarized version of MNIST~\cite{deng2012mnist}), then not all $2^{784}$ inputs would be equally likely to appear; those that \emph{look like digits} would be much more likely.
When considering a partial instance $\vy$, we define $\D_{\vy}$ as the distribution $\D$ conditioned on being a completion of $\vy$, which formally means that 
\[
	\D_{\vy}(\vz) = \frac{\D(\vz)}{\sum_{\vw \in \comp(\vy)} \D(\vw)},
\]
for any $\vz \in \comp(\vy)$, and $\D_{\vy}(\vv) = 0$ if $\vv \not\in \comp(\vy)$.
 
With this notation we can define $\delta$-sufficient reasons, also known as $\delta$-relevant set~\cite{Izza2021EfficientEW}.
\begin{definition}[\cite{Waldchen_MacDonald_Hauch_Kutyniok_2021}]
	For any $\delta \in [0, 1]$, a $\delta$-sufficient reason ($\delta$-SR) for an instance $\vx$, is a partial instance $\vy \subseteq \vx$ such that
	\[
		\Pr_{\vz \sim \D_{\vy}}\big[\M(\vz) = \M(\vx)\big] \geq \delta.
	\]
	\label{def:delta-SR}
\end{definition}
%
Note immediately that~\Cref{def:delta-SR} and~\Cref{def:sufficient-reason} coincide when $\delta = 1$.
\subsection{The size of $\delta$-SRs}

Interestingly, even a $0.999999$-SR can be arbitrarily smaller, in terms of defined features, than the smallest sufficient reason (i.e., $1$-SR) for a pair $(\M, \vx)$, even when $\M$ is a linear model, as we will illustrate in~\Cref{ex:delta-sr-size}. Before providing the example, let us define linear models and a concentration bound that will be used throughout the paper.

\begin{definition}
	A (binary) linear model $\Lin$ of dimension $d$ is a pair $(\vw, t)$, where $\vw \in \mathbb{Q}^d$ and $t \in \mathbb{Q}$. Its classification over an instance $\vx$ is defined simply as 
	\[
		\Lin(\vx) = \begin{cases}
			1 & \text{if } \vx \cdot \vw \geq t\\
			0 & \text{otherwise}.
		\end{cases}
	\]
	\label{def:linear-models}
\end{definition}

\begin{lemma}[Chernoff-Hoeffding bound]
Let $X$ be a finite sum of independent Bernoulli variables, with $\E[X] = \mu > 0$. Then, for any $t \geq 0$, we have
\[
\Pr \Big[ \left|X - \mu\right| \geq t \Big] \leq 2\exp\left(\frac{-t^2}{3 \mu} \right).
\] 

\label{lemma:chernoff}	
\end{lemma}

\begin{example}

Consider a linear model $L$ of dimension $d= 1000$ with parameters $t = 1250$ and
$$
	\vw = (
		1000, \, 1, \, 1, \,  1, \,  1, \,  \ldots, \, 1
	).
$$
Let the instance $\vx$ be 
	$(
		1, \, 1, \, 1, \,  1, \,  1, \,  \ldots, \, 1
	),$ so that clearly $\Lin(\vx) = 1$.
One can easily see that any $1$-SR for $\vx$ under $\Lin$ has size $251$, as it must include the first feature and any $250$ other features.
However, if we consider $\D$ to be a uniform distribution, and $\vy = (
		1, \, \bot, \, \bot, \,  \bot,  \ldots, \, \bot
	)$, then a simple application of \Cref{lemma:chernoff} gives that
\[
	\Pr_{\vz \sim \D(\vy)}\Big[ \Lin(\vz) = 1\Big] \geq  0.999999.\]
	This suggests that we might say $\Lin(\vx) = 1$ ``because'' $x_1 = 1$; formally, $\vy$ is a $0.999999$-SR, and $251$ times smaller than any $1$-SR for $\Lin(\vx)$.
\label{ex:delta-sr-size}
\end{example}

In general, if we let $k^\star(\M, \vx, \delta)$ be the size of the smallest $\delta$-SR for $(\M, \vx)$ under the uniform distribution, we can prove the following simple proposition.

\begin{proposition}
	Even when restricted to the class of linear models, or decision trees, for any $\delta \in (0, 1]$, and any $\varepsilon > 0$, there are pairs $(\M, \vx)$  such that
	\[ 
		\frac{k^\star(\M, \vx, \delta)}{k^\star(\M, \vx, \delta-\varepsilon)} = \Omega(d).
	\]
\end{proposition}